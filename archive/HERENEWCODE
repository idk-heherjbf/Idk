import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import re
import json
import os
from collections import Counter

class SimpleWordTokenizer:
    """Word-level tokenizer optimized for command-line data"""
    def __init__(self):
        self.special_tokens = ["<PAD>", "<UNK>", "<BOS>", "<EOS>"]
        self.word_to_id = {}
        self.id_to_word = {}
        self.vocab_size = 0
        
    def build_vocab(self, texts, min_freq=2):
        """Build vocabulary from texts with minimum frequency threshold"""
        word_counts = Counter()
        
        for text in texts:
            words = self.tokenize_text(text)
            word_counts.update(words)
        
        # Start with special tokens
        vocab = self.special_tokens.copy()
        
        # Add words above frequency threshold
        for word, count in word_counts.most_common():
            if count >= min_freq:
                vocab.append(word)
            
        # Limit vocabulary size for computational efficiency
        vocab = vocab[:2000]  # Much smaller vocab
        
        self.word_to_id = {word: i for i, word in enumerate(vocab)}
        self.id_to_word = {i: word for word, i in self.word_to_id.items()}
        self.vocab_size = len(vocab)
        
        print(f"Built vocabulary with {self.vocab_size} tokens")
        print(f"Most common words: {list(word_counts.most_common(20))}")
        
    def tokenize_text(self, text):
        """Split text into meaningful tokens for command-line context"""
        text = text.lower().strip()
        
        # Preserve important command-line patterns
        # Keep flags like -p, -v, etc. together
        text = re.sub(r'(-[a-zA-Z0-9]+)', r' \1 ', text)
        
        # Keep IP addresses together
        text = re.sub(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', r' \1 ', text)
        
        # Keep common command patterns together
        text = re.sub(r'(<[^>]+>)', r' \1 ', text)  # <target>, <url>, etc.
        
        # Split on whitespace and punctuation, but preserve meaningful units
        words = re.findall(r'<[^>]+>|--?[a-zA-Z0-9]+|\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}|\w+|[^\w\s]', text)
        
        return [w for w in words if w.strip()]
    
    def encode(self, text):
        """Convert text to token IDs"""
        words = self.tokenize_text(text)
        return [self.word_to_id.get(word, self.word_to_id["<UNK>"]) for word in words]
    
    def decode(self, token_ids):
        """Convert token IDs back to text"""
        words = [self.id_to_word.get(id, "<UNK>") for id in token_ids]
        # Remove special tokens except for debugging
        words = [w for w in words if w not in ["<PAD>", "<BOS>", "<EOS>"]]
        return " ".join(words)

class SimpleLSTMModel(nn.Module):
    """Much simpler LSTM-based model instead of Transformer"""
    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.decoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, src, tgt):
        # Encode source
        src_embed = self.embedding(src)
        encoder_out, (h_n, c_n) = self.encoder(src_embed)
        
        # Decode target
        tgt_embed = self.embedding(tgt)
        decoder_out, _ = self.decoder(tgt_embed, (h_n, c_n))
        
        # Project to vocabulary
        output = self.output_projection(self.dropout(decoder_out))
        return output

class CommandDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_len=32):
        self.samples = []
        self.tokenizer = tokenizer
        self.max_len = max_len
        
        bos_id = tokenizer.word_to_id["<BOS>"]
        eos_id = tokenizer.word_to_id["<EOS>"]
        pad_id = tokenizer.word_to_id["<PAD>"]
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                if '?' in line and not line.startswith('?'):
                    parts = line.split('?', 1)
                    question = parts[0].strip() + '?'
                    answer = parts[1].strip()
                    
                    if answer.lower().startswith('a '):
                        answer = answer[2:].strip()
                    
                    if answer:
                        # Encode and truncate
                        q_ids = tokenizer.encode(question)[:max_len-1]
                        a_ids = tokenizer.encode(answer)[:max_len-2]
                        
                        # Add special tokens
                        a_ids = [bos_id] + a_ids + [eos_id]
                        
                        # Pad to max length
                        q_ids += [pad_id] * (max_len - len(q_ids))
                        a_ids += [pad_id] * (max_len - len(a_ids))
                        
                        self.samples.append((q_ids, a_ids))
        
        print(f"Loaded {len(self.samples)} training samples")
        
        # Debug: show first few samples
        for i, (q, a) in enumerate(self.samples[:3]):
            print(f"Sample {i}:")
            print(f"  Q: {tokenizer.decode([x for x in q if x != pad_id])}")
            print(f"  A: {tokenizer.decode([x for x in a if x != pad_id])}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        src, tgt = self.samples[idx]
        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)

def train_simple_model():
    """Training function with much simpler approach"""
    
    # Paths
    data_path = "archive/data/cleaned_shell_logs.txt"
    model_path = "simple_model.pt"
    
    print("Building tokenizer...")
    tokenizer = SimpleWordTokenizer()
    
    # Read all texts for vocabulary building
    texts = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if '?' in line and not line.startswith('?'):
                parts = line.split('?', 1)
                question = parts[0].strip() + '?'
                answer = parts[1].strip()
                if answer.lower().startswith('a '):
                    answer = answer[2:].strip()
                if answer:
                    texts.extend([question, answer])
    
    tokenizer.build_vocab(texts, min_freq=1)  # Include all words for small dataset
    
    # Create dataset and model
    dataset = CommandDataset(data_path, tokenizer)
    if len(dataset) == 0:
        print("ERROR: No training data found!")
        return
    
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SimpleLSTMModel(tokenizer.vocab_size).to(device)
    
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_id["<PAD>"])
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print(f"Training on {device} with {len(dataset)} samples...")
    
    # Training loop
    model.train()
    for epoch in range(50):
        total_loss = 0
        num_batches = 0
        
        for src, tgt in dataloader:
            src, tgt = src.to(device), tgt.to(device)
            
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
            
            optimizer.zero_grad()
            
            logits = model(src, tgt_input)
            loss = criterion(logits.reshape(-1, tokenizer.vocab_size), tgt_output.reshape(-1))
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / max(1, num_batches)
        print(f"Epoch {epoch+1}/50 - Loss: {avg_loss:.4f}")
        
        # Test generation every 10 epochs
        if (epoch + 1) % 10 == 0:
            test_question = "How do I scan ports on a target?"
            response = generate_response(model, tokenizer, test_question, device)
            print(f"Test: {test_question} -> {response}")
    
    # Save model and tokenizer
    torch.save({
        'model_state_dict': model.state_dict(),
        'tokenizer_word_to_id': tokenizer.word_to_id,
        'tokenizer_id_to_word': tokenizer.id_to_word,
        'vocab_size': tokenizer.vocab_size
    }, model_path)
    
    print(f"Model saved to {model_path}")
    return model, tokenizer

def generate_response(model, tokenizer, question, device, max_len=30):
    """Generate response using the trained model"""
    model.eval()
    
    with torch.no_grad():
        # Encode question
        q_ids = tokenizer.encode(question)[:31]  # Leave room for padding
        q_ids += [tokenizer.word_to_id["<PAD>"]] * (32 - len(q_ids))
        src = torch.tensor([q_ids], dtype=torch.long).to(device)
        
        # Start with BOS token
        bos_id = tokenizer.word_to_id["<BOS>"]
        eos_id = tokenizer.word_to_id["<EOS>"]
        pad_id = tokenizer.word_to_id["<PAD>"]
        
        output_ids = [bos_id]
        tgt = torch.tensor([output_ids], dtype=torch.long).to(device)
        
        for _ in range(max_len):
            logits = model(src, tgt)
            next_token = torch.argmax(logits[0, -1]).item()
            
            if next_token == eos_id or next_token == pad_id:
                break
                
            output_ids.append(next_token)
            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)
        
        # Remove BOS token and decode
        response_ids = output_ids[1:]  # Remove BOS
        return tokenizer.decode(response_ids)

if __name__ == "__main__":
    model, tokenizer = train_simple_model()
    
    # Test the model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    test_questions = [
        "How do I scan ports on a target?",
        "How do I enumerate SMB shares?",
        "How do I check for SQL injection?",
        "How do I find subdomains?"
    ]
    
    print("\n=== Testing Model ===")
    for question in test_questions:
        response = generate_response(model, tokenizer, question, device)
        print(f"Q: {question}")
        print(f"A: {response}")
        print()import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import re
import json
import os
from collections import Counter

class SimpleWordTokenizer:
    """Word-level tokenizer optimized for command-line data"""
    def __init__(self):
        self.special_tokens = ["<PAD>", "<UNK>", "<BOS>", "<EOS>"]
        self.word_to_id = {}
        self.id_to_word = {}
        self.vocab_size = 0
        
    def build_vocab(self, texts, min_freq=2):
        """Build vocabulary from texts with minimum frequency threshold"""
        word_counts = Counter()
        
        for text in texts:
            words = self.tokenize_text(text)
            word_counts.update(words)
        
        # Start with special tokens
        vocab = self.special_tokens.copy()
        
        # Add words above frequency threshold
        for word, count in word_counts.most_common():
            if count >= min_freq:
                vocab.append(word)
            
        # Limit vocabulary size for computational efficiency
        vocab = vocab[:2000]  # Much smaller vocab
        
        self.word_to_id = {word: i for i, word in enumerate(vocab)}
        self.id_to_word = {i: word for word, i in self.word_to_id.items()}
        self.vocab_size = len(vocab)
        
        print(f"Built vocabulary with {self.vocab_size} tokens")
        print(f"Most common words: {list(word_counts.most_common(20))}")
        
    def tokenize_text(self, text):
        """Split text into meaningful tokens for command-line context"""
        text = text.lower().strip()
        
        # Preserve important command-line patterns
        # Keep flags like -p, -v, etc. together
        text = re.sub(r'(-[a-zA-Z0-9]+)', r' \1 ', text)
        
        # Keep IP addresses together
        text = re.sub(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', r' \1 ', text)
        
        # Keep common command patterns together
        text = re.sub(r'(<[^>]+>)', r' \1 ', text)  # <target>, <url>, etc.
        
        # Split on whitespace and punctuation, but preserve meaningful units
        words = re.findall(r'<[^>]+>|--?[a-zA-Z0-9]+|\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}|\w+|[^\w\s]', text)
        
        return [w for w in words if w.strip()]
    
    def encode(self, text):
        """Convert text to token IDs"""
        words = self.tokenize_text(text)
        return [self.word_to_id.get(word, self.word_to_id["<UNK>"]) for word in words]
    
    def decode(self, token_ids):
        """Convert token IDs back to text"""
        words = [self.id_to_word.get(id, "<UNK>") for id in token_ids]
        # Remove special tokens except for debugging
        words = [w for w in words if w not in ["<PAD>", "<BOS>", "<EOS>"]]
        return " ".join(words)

class SimpleLSTMModel(nn.Module):
    """Much simpler LSTM-based model instead of Transformer"""
    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.decoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.output_projection = nn.Linear(hidden_size, vocab_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, src, tgt):
        # Encode source
        src_embed = self.embedding(src)
        encoder_out, (h_n, c_n) = self.encoder(src_embed)
        
        # Decode target
        tgt_embed = self.embedding(tgt)
        decoder_out, _ = self.decoder(tgt_embed, (h_n, c_n))
        
        # Project to vocabulary
        output = self.output_projection(self.dropout(decoder_out))
        return output

class CommandDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_len=32):
        self.samples = []
        self.tokenizer = tokenizer
        self.max_len = max_len
        
        bos_id = tokenizer.word_to_id["<BOS>"]
        eos_id = tokenizer.word_to_id["<EOS>"]
        pad_id = tokenizer.word_to_id["<PAD>"]
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                if '?' in line and not line.startswith('?'):
                    parts = line.split('?', 1)
                    question = parts[0].strip() + '?'
                    answer = parts[1].strip()
                    
                    if answer.lower().startswith('a '):
                        answer = answer[2:].strip()
                    
                    if answer:
                        # Encode and truncate
                        q_ids = tokenizer.encode(question)[:max_len-1]
                        a_ids = tokenizer.encode(answer)[:max_len-2]
                        
                        # Add special tokens
                        a_ids = [bos_id] + a_ids + [eos_id]
                        
                        # Pad to max length
                        q_ids += [pad_id] * (max_len - len(q_ids))
                        a_ids += [pad_id] * (max_len - len(a_ids))
                        
                        self.samples.append((q_ids, a_ids))
        
        print(f"Loaded {len(self.samples)} training samples")
        
        # Debug: show first few samples
        for i, (q, a) in enumerate(self.samples[:3]):
            print(f"Sample {i}:")
            print(f"  Q: {tokenizer.decode([x for x in q if x != pad_id])}")
            print(f"  A: {tokenizer.decode([x for x in a if x != pad_id])}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        src, tgt = self.samples[idx]
        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)

def train_simple_model():
    """Training function with much simpler approach"""
    
    # Paths
    data_path = "archive/data/cleaned_shell_logs.txt"
    model_path = "simple_model.pt"
    
    print("Building tokenizer...")
    tokenizer = SimpleWordTokenizer()
    
    # Read all texts for vocabulary building
    texts = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if '?' in line and not line.startswith('?'):
                parts = line.split('?', 1)
                question = parts[0].strip() + '?'
                answer = parts[1].strip()
                if answer.lower().startswith('a '):
                    answer = answer[2:].strip()
                if answer:
                    texts.extend([question, answer])
    
    tokenizer.build_vocab(texts, min_freq=1)  # Include all words for small dataset
    
    # Create dataset and model
    dataset = CommandDataset(data_path, tokenizer)
    if len(dataset) == 0:
        print("ERROR: No training data found!")
        return
    
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SimpleLSTMModel(tokenizer.vocab_size).to(device)
    
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_id["<PAD>"])
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print(f"Training on {device} with {len(dataset)} samples...")
    
    # Training loop
    model.train()
    for epoch in range(50):
        total_loss = 0
        num_batches = 0
        
        for src, tgt in dataloader:
            src, tgt = src.to(device), tgt.to(device)
            
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
            
            optimizer.zero_grad()
            
            logits = model(src, tgt_input)
            loss = criterion(logits.reshape(-1, tokenizer.vocab_size), tgt_output.reshape(-1))
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / max(1, num_batches)
        print(f"Epoch {epoch+1}/50 - Loss: {avg_loss:.4f}")
        
        # Test generation every 10 epochs
        if (epoch + 1) % 10 == 0:
            test_question = "How do I scan ports on a target?"
            response = generate_response(model, tokenizer, test_question, device)
            print(f"Test: {test_question} -> {response}")
    
    # Save model and tokenizer
    torch.save({
        'model_state_dict': model.state_dict(),
        'tokenizer_word_to_id': tokenizer.word_to_id,
        'tokenizer_id_to_word': tokenizer.id_to_word,
        'vocab_size': tokenizer.vocab_size
    }, model_path)
    
    print(f"Model saved to {model_path}")
    return model, tokenizer

def generate_response(model, tokenizer, question, device, max_len=30):
    """Generate response using the trained model"""
    model.eval()
    
    with torch.no_grad():
        # Encode question
        q_ids = tokenizer.encode(question)[:31]  # Leave room for padding
        q_ids += [tokenizer.word_to_id["<PAD>"]] * (32 - len(q_ids))
        src = torch.tensor([q_ids], dtype=torch.long).to(device)
        
        # Start with BOS token
        bos_id = tokenizer.word_to_id["<BOS>"]
        eos_id = tokenizer.word_to_id["<EOS>"]
        pad_id = tokenizer.word_to_id["<PAD>"]
        
        output_ids = [bos_id]
        tgt = torch.tensor([output_ids], dtype=torch.long).to(device)
        
        for _ in range(max_len):
            logits = model(src, tgt)
            next_token = torch.argmax(logits[0, -1]).item()
            
            if next_token == eos_id or next_token == pad_id:
                break
                
            output_ids.append(next_token)
            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)
        
        # Remove BOS token and decode
        response_ids = output_ids[1:]  # Remove BOS
        return tokenizer.decode(response_ids)

if __name__ == "__main__":
    model, tokenizer = train_simple_model()
    
    # Test the model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    test_questions = [
        "How do I scan ports on a target?",
        "How do I enumerate SMB shares?",
        "How do I check for SQL injection?",
        "How do I find subdomains?"
    ]
    
    print("\n=== Testing Model ===")
    for question in test_questions:
        response = generate_response(model, tokenizer, question, device)
        print(f"Q: {question}")
        print(f"A: {response}")
        print()